# GEMM Kernel Analysis

## Overview

The GEMM (General Matrix Multiply) kernels in this project implement high-performance matrix multiplication operations using MLX's `fast.metal_kernel` API. These kernels are foundational for xLSTM operations and demonstrate advanced Metal optimization techniques.

## Kernel Variants

### 1. `gemm_av`: Matrix-Matrix Multiply
**Operation**: `C = A × V` where A is (m,n) and V is (n,k) → C is (m,k)

**Use Case**: Forward pass projections in neural networks

### 2. `gemm_at_b`: Transposed Matrix Multiply  
**Operation**: `Z = Aᵀ × B` where A is (m,n) and B is (m,k) → Z is (n,k)

**Use Case**: Gradient computation and weight updates in backpropagation

## Architecture & Design Patterns

### Core Design Philosophy

Following MLX's `fast.metal_kernel` contract:
- **Body-only Metal source**: MLX auto-generates kernel signatures
- **Dynamic shapes via buffers**: Pass [m, n, k] through buffer to avoid recompilation
- **2D tiling with cooperative loading**: Multiple threads load shared data
- **Double barrier synchronization**: After loads and after accumulation

### Tiling Strategy

```metal
// Example: 16x16 tile for gemm_av
threadgroup float Asub[T][T + PAD];  // Tile from A
threadgroup float Vsub[T][T + PAD];  // Tile from V

// Each threadgroup processes one output tile
// Each thread accumulates one output element
```

**Key Insight**: Tiles stored in threadgroup shared memory (32KB limit) with optional +1 padding to reduce bank conflicts.

## Optimization Techniques

### 1. Threadgroup Shared Memory (32KB)

**Without Tiling** (Naive):
```metal
// Each thread reads from global memory for every multiply
float result = 0.0f;
for (int i = 0; i < n; i++) {
    result += A[row * n + i] * V[i * k + col];  // 2 global loads per iteration
}
```

**With Tiling**:
```metal
// Load tile into shared memory once
threadgroup float Asub[16][16];
threadgroup float Vsub[16][16];

// Cooperative loading (coalesced)
Asub[ty][tx] = A[...];
Vsub[ty][tx] = V[...];
threadgroup_barrier(mem_flags::mem_threadgroup);

// Multiple reuses from fast shared memory
for (uint p = 0; p < 16; p++) {
    acc = fma(Asub[ty][p], Vsub[p][tx], acc);
}
```

**Impact**: 
- Reduces global memory accesses by 16x (for 16x16 tiles)
- Shared memory bandwidth: ~400 GB/s vs ~200 GB/s global memory
- Each value loaded once, reused 16 times

### 2. Cooperative Loading with Unique Writers

**Critical Pattern**: One writer per shared memory location

```metal
// CORRECT: Unique writer per tile cell
uint tx = thread_position_in_threadgroup.x;  // 0..T-1
uint ty = thread_position_in_threadgroup.y;  // 0..T-1

// Each thread loads exactly one element
Asub[ty][tx] = A[row * n + a_col];  // No race condition
Vsub[ty][tx] = V[v_row * k + col];

threadgroup_barrier(mem_flags::mem_threadgroup);  // Synchronize before read
```

**From GEMM_TILING_ATTEMPTS.md**:
> Early AT_B loaders allowed multiple threads to write the same threadgroup tile address, causing races and incorrect results.

**Fix**: Carefully map thread indices to ensure unique (ty, tx) pairs write unique tile locations.

### 3. Double Barrier Synchronization

Two barriers per K-tile iteration are **essential**:

```metal
for (int t = 0; t < ntiles; ++t) {
    // Load tile
    Asub[ty][tx] = A[...];
    Vsub[ty][tx] = V[...];
    
    threadgroup_barrier(mem_flags::mem_threadgroup);  // Barrier 1: After load
    
    // Accumulate (all threads read shared data)
    for (uint p = 0; p < T; ++p) {
        acc = fma(Asub[ty][p], Vsub[p][tx], acc);
    }
    
    threadgroup_barrier(mem_flags::mem_threadgroup);  // Barrier 2: Before next load
}
```

**Why Two Barriers**:
1. **Barrier 1**: Ensure all loads complete before any thread reads (prevents reading uninitialized data)
2. **Barrier 2**: Ensure all reads complete before overwriting in next iteration (prevents read-after-write hazards)

**Visibility Guarantee**: Barriers with `mem_flags::mem_threadgroup` ensure memory operations are visible across SIMD groups within the threadgroup.

### 4. Padding to Avoid Bank Conflicts

```metal
const uint PAD = 1;  // Optional padding
threadgroup float Asub[T][T + PAD];  // Extra column to shift addresses
```

**Bank Conflict Problem**: 
- Shared memory organized in banks (typically 32)
- When multiple threads access same bank simultaneously → serialization
- Pattern: stride-16 or stride-32 accesses can all hit same bank

**Solution**: Add +1 padding to shift column addresses
- `Asub[0][16]` and `Asub[1][16]` now in different banks
- Controlled by `XLSTM_GEMM_PAD=1` environment variable

**Impact**: Up to 2x speedup on patterns with bank conflicts (varies by GPU)

### 5. FMA (Fused Multiply-Add) Instructions

```metal
// Standard: 2 instructions
float mul = a * b;
float result = c + mul;

// FMA: 1 instruction, higher precision
float result = fma(a, b, c);  // c + (a * b) with single rounding
```

**Benefits**:
- 2x throughput (1 instruction vs 2)
- Better numerical precision (one rounding step instead of two)
- Lower register pressure

### 6. Grid Configuration for MLX

**Critical Understanding** from GEMM_TILING_ATTEMPTS.md:

> We initially treated `grid` as the number of threadgroups rather than total threads (MLX uses dispatchThreads semantics). This under-dispatched work.

**Correct Pattern**:
```python
T = 16  # Tile size
m, n, k = A.shape[0], A.shape[1], V.shape[1]

# Grid = TOTAL THREADS (not threadgroups!)
gx = ((k + T - 1) // T) * T  # Round up to full tiles
gy = ((m + T - 1) // T) * T
grid = (gx, gy, 1)

# Threadgroup = threads per group
threadgroup = (T, T, 1)
```

**Result**:
- `threadgroup_position_in_grid` enumerates tile indices
- `thread_position_in_threadgroup` covers local tile coordinates
- Each thread computes one output element

## Advanced: Double Buffering

Optional optimization for hiding memory latency:

```metal
// Two sets of tiles (ping-pong buffers)
threadgroup float Asub0[T][T + PAD];
threadgroup float Asub1[T][T + PAD];
threadgroup float Vsub0[T][T + PAD];
threadgroup float Vsub1[T][T + PAD];

// Prefetch first tile into buffer 0
// ... load into Asub0/Vsub0 ...
threadgroup_barrier(mem_flags::mem_threadgroup);

bool use0 = true;
for (int t = 0; t < ntiles; ++t) {
    // Prefetch NEXT tile into the other buffer
    if (t + 1 < ntiles) {
        if (use0) { /* load into Asub1/Vsub1 */ }
        else      { /* load into Asub0/Vsub0 */ }
    }
    
    // Compute on CURRENT tile
    if (use0) { /* accumulate from Asub0/Vsub0 */ }
    else      { /* accumulate from Asub1/Vsub1 */ }
    
    threadgroup_barrier(mem_flags::mem_threadgroup);
    if (t + 1 < ntiles) {
        threadgroup_barrier(mem_flags::mem_threadgroup);  // Wait for prefetch
        use0 = !use0;  // Swap buffers
    }
}
```

**Benefit**: While computing on tile N, prefetch tile N+1 in parallel
**Cost**: 2x shared memory usage (8KB → 16KB for 16x16 tiles)
**When to use**: Large matrices where memory latency is significant

## Device-Specific Tuning

### Tile Size Selection

**M3 Optimized** (from code):
```python
if "m3" in device_name.lower():
    gemm_av:  TM=32, T=8   (32x8 tiles)
    gemm_at_b: TN=8, TK=32, TI=16
else:
    # Default
    gemm_av:  TM=16, T=16  (16x16 tiles)
    gemm_at_b: TN=16, TK=16, TI=16
```

**Rationale**:
- M3 GPU has different cache/shared memory characteristics
- Asymmetric tiles (32x8) can better match execution units
- Needs empirical benchmarking per device

### Environment Variable Overrides

```bash
# Override tile sizes
export XLSTM_GEMM_TILE_AV="32x8"
export XLSTM_GEMM_TILE_ATB="8x32"

# Enable padding
export XLSTM_GEMM_PAD=1

# Align to execution width
export XLSTM_GEMM_ALIGN_EXECW=1

# Enable double buffering
export XLSTM_GEMM_DB=1
```

### Execution Width Alignment

```python
exec_width = metal.device_info().get("threadExecutionWidth", 32)
if align_execw and (exec_width * exec_width) <= 1024:
    T = exec_width  # Use 32x32 if exec_width=32
```

**Idea**: Align tile size to hardware SIMD width for better occupancy
**Risk**: May use too much shared memory (32x32 = 4KB per tile)

## Performance Characteristics

### Memory Bandwidth Analysis

**Naive Implementation** (no tiling):
- Loads: 2 × m × n × k (each element of A and V accessed k or m times)
- Bandwidth: ~20 GB/s (poor reuse, random access pattern)

**Tiled Implementation** (16x16 tiles):
- Loads: 2 × m × n × k / T (each element loaded once per T multiplications)
- Bandwidth: ~150-200 GB/s (coalesced loads, shared memory reuse)

**Speedup**: 7-10x on large matrices

### Arithmetic Intensity

```
Operations: m × n × k × 2 (multiply + add)
Memory:     2 × m × n × k × 4 bytes (with tiling) / T
Intensity:  2 × T / 4 = T/2 FLOPs/byte

For T=16: 8 FLOPs/byte (compute-bound territory)
For T=8:  4 FLOPs/byte (memory-bound)
```

**Implication**: Larger tiles improve arithmetic intensity, shifting from memory-bound to compute-bound.

## Lessons from GEMM_TILING_ATTEMPTS.md

### Mistake 1: Grid Semantics
**Wrong**: Treated grid as threadgroup count
**Right**: Grid = total threads in MLX

### Mistake 2: Data Races
**Wrong**: Multiple threads writing same shared memory location
**Right**: Unique writer per (ty, tx) coordinate

### Mistake 3: Missing Barriers
**Wrong**: Single barrier or no barriers
**Right**: Two barriers per iteration (after load, after compute)

### Mistake 4: Non-contiguous Memory
**Wrong**: Assumed arbitrary strides
**Right**: Use `ensure_row_contiguous=True` and validate layout

## Testing Strategy

From `test_gemm_kernels.py`:

1. **Correctness**: Compare against `mx.matmul()` reference
2. **Odd shapes**: Test partial tiles (33×29×31)
3. **Tiny inputs**: Catch indexing bugs (2×3 matrices)
4. **Multiple tile sizes**: Validate all configurations
5. **Relative error**: `max(abs(custom - reference)) < 1e-5`

```python
def check(shape=(64, 128, 32), tiles=(16, 16)):
    A = mx.random.normal(shape=(m, n))
    V = mx.random.normal(shape=(n, k))
    
    B_reference = mx.matmul(A, V)
    B_custom = gemm_av(A, V)
    
    diff = float(mx.max(mx.abs(B_reference - B_custom)))
    assert diff < 1e-5, f"Failed with diff={diff}"
```

## Integration with xLSTM

These GEMM kernels are used in:
- **mLSTM memory updates**: Matrix outer products and projections
- **sLSTM state transitions**: Linear transformations
- **Attention mechanisms**: Q×Kᵀ and attention×V
- **FFN layers**: Weight matrix multiplications

**Why Custom Kernels**:
- MLX's built-in matmul is optimized for general cases
- xLSTM has specific patterns (e.g., frequent Aᵀ×B in gradients)
- Custom kernels allow fusion opportunities (e.g., GEMM + activation)
- Fine-grained control over memory layout for sequential dependencies

## Comparison to Variable Quantization Kernel

| Aspect | GEMM Kernels | Variable Quantization |
|--------|--------------|----------------------|
| **Memory Pattern** | Coalesced loads/stores | Coalesced vectorized (float4) |
| **Shared Memory** | Essential (32KB) | Optional (4KB blocks) |
| **Barriers** | 2 per K-tile iteration | 2 per block (load/store) |
| **Arithmetic Intensity** | High (8 FLOPs/byte for T=16) | Low (0.5 FLOPs/byte) |
| **Bottleneck** | Balanced compute/memory | Pure memory bandwidth |
| **Optimization Focus** | Tile reuse | Memory coalescing |
| **Speedup vs Naive** | 7-10x | 26x |

## Key Takeaways

1. **Tiling is essential**: 16x reduction in global memory traffic
2. **Barriers guarantee correctness**: Don't skip them for performance
3. **Unique writers**: Avoid data races in shared memory
4. **Coalesced access**: Adjacent threads access adjacent memory
5. **FMA instructions**: 2x throughput, better precision
6. **Device-specific tuning**: One size doesn't fit all GPUs
7. **MLX grid semantics**: Grid = total threads, not threadgroups
8. **Padding helps**: +1 column can eliminate bank conflicts
9. **Double buffering**: Hide latency on large matrices
10. **Test thoroughly**: Odd shapes, tiny inputs, multiple tile sizes

## Future Optimization Opportunities

1. **Vectorized loads**: Use float4 for 4x memory bandwidth
2. **Warp-level primitives**: Simdgroup operations for reduction
3. **Persistent kernels**: Reuse threadgroups across tiles
4. **Mixed precision**: fp16 accumulation with fp32 output
5. **Kernel fusion**: GEMM + bias + activation in single kernel
6. **Async copy**: Overlap compute and memory on newer GPUs
7. **Register blocking**: Further tile output in registers
8. **Non-square tiles**: Optimize for tall/wide matrices

## References

- MLX fast.metal_kernel documentation
- Metal Shading Language Specification
- GEMM_TILING_ATTEMPTS.md (lessons learned)
- test_gemm_kernels.py (validation)
- gemm_tile_bench.py (performance tuning)

---

**Status**: Production-ready kernels used in xLSTM implementation. Validated against MLX reference implementation with <1e-5 relative error across all tested shapes.

