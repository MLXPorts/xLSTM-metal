# PyTorch Metal Soft Cap

PyTorch implementation of soft cap using custom Metal kernels via JIT compilation.

## Overview

This module provides a Metal-accelerated soft cap operation for PyTorch tensors on Apple Silicon (MPS backend). The soft cap function is: `output = cap_value * tanh(input / cap_value)`.

## Implementation

- **Backend**: Objective-C++ Metal wrapper (`pytorch_mm/mlstm_metal_backend.mm`)
- **Shader**: Metal kernel (`pytorch_mm/mlstm_kernels.metal`)
- **Python Interface**: JIT-compiled wrapper with automatic path resolution

## Usage

```python
import torch
from softcap import metal_soft_cap

# Create tensor on MPS device
x = torch.tensor([0.5, 1.0, 2.0, 10.0], device="mps", dtype=torch.float32)

# Apply soft cap with cap_value=5.0
y = metal_soft_cap(x, cap_value=5.0)

# Output: tensor([0.4983, 0.9869, 1.8997, 4.9999], device='mps:0')
```

## Features

- **JIT Compilation**: Automatically compiles Metal backend on first use
- **Path Resolution**: Finds backend and shader files automatically
- **Caching**: Compiled extension cached by PyTorch for fast subsequent loads
- **Shape Preservation**: Works with any tensor shape (1D, 2D, etc.)
- **MPS Native**: Direct Metal buffer access, no CPU transfers

## Requirements

- PyTorch with MPS support
- macOS with Apple Silicon (M1/M2/M3)
- Xcode Command Line Tools

## Testing

```bash
cd kernel_development/soft_cap/pytorch_metal_compiled
python test_softcap.py
```

**Expected Output**:
```
======================================================================
PyTorch Metal Soft Cap Test
======================================================================

✓ MPS is available

Test 1: Basic soft cap with cap=5.0
  Input:  [0.0, 5.0, 50.0, -50.0, 0.5, -0.5]
  Output: [0.0, 3.808, 5.0, -5.0, 0.498, -0.498]
  Expected: [0.0, 3.808, 5.0, -5.0, 0.5, -0.5]
  ✓ All values correct

Test 2: 2D tensor
  Input shape: torch.Size([2, 2])
  Output shape: torch.Size([2, 2])
  ✓ Multi-dimensional works

Test 3: Large tensor (10000 elements)
  ✓ All values bounded by cap

======================================================================
All tests passed!
======================================================================
```

## Technical Details

### JIT Compilation

On first import, PyTorch's `cpp_extension.load()`:
1. Compiles `mlstm_metal_backend.mm` with Metal frameworks
2. Links against PyTorch libraries
3. Caches result in `~/.cache/torch_extensions/`
4. Subsequent loads use cached binary (instant)

### Metal Kernel

Located in `mlstm_kernels.metal`:

```metal
kernel void soft_cap_kernel(
    device float* input [[buffer(0)]],
    device float* output [[buffer(1)]],
    constant float& cap_value [[buffer(2)]],
    constant uint& size [[buffer(3)]],
    uint id [[thread_position_in_grid]]
) {
    if (id >= size) return;
    float val = input[id];
    output[id] = cap_value * tanh(val / cap_value);
}
```

### Buffer Mapping

The backend uses `getMTLBufferStorage()` to extract Metal buffers directly from PyTorch MPS tensors, avoiding CPU transfers:

```cpp
id<MTLBuffer> getMTLBufferStorage(const torch::Tensor& tensor) {
    return __builtin_bit_cast(id<MTLBuffer>, tensor.storage().data());
}
```

## Performance

- **No CPU transfers**: Direct GPU-to-GPU operation
- **Vectorized**: Metal shader processes elements in parallel
- **Minimal overhead**: JIT compilation cached after first use
- **Large tensors**: Efficiently handles 10K+ elements

## Comparison to MLX Version

| Feature | PyTorch Metal | MLX Fast Kernel |
|---------|---------------|-----------------|
| Backend | PyTorch MPS | MLX Metal |
| Compilation | JIT (first use) | Ahead-of-time via `.compile()` |
| Buffer Access | Direct via `__builtin_bit_cast` | MLX managed |
| Framework | PyTorch ecosystem | MLX ecosystem |
| Use Case | PyTorch models on MPS | MLX models |

## Troubleshooting

### "MPS not available"
- Ensure you have PyTorch with MPS support installed
- Run on macOS with Apple Silicon

### "Metal backend source not found"
- Verify `pytorch_mm/mlstm_metal_backend.mm` exists
- Check file paths in `softcap.py`

### Compilation Errors
- Install Xcode Command Line Tools: `xcode-select --install`
- Check Metal framework availability

### "function 'soft_cap_kernel' not found"
- Ensure `mlstm_kernels.metal` contains `soft_cap_kernel` function
- Check shader source is being read correctly

## Files

- `softcap.py` - Python interface with JIT compilation
- `test_softcap.py` - Comprehensive test suite
- `../pytorch_mm/mlstm_metal_backend.mm` - Objective-C++ Metal wrapper
- `../pytorch_mm/mlstm_kernels.metal` - Metal shader source

## Integration

Use in PyTorch models:

```python
class MyModel(torch.nn.Module):
    def forward(self, x):
        # ... model operations ...
        x = metal_soft_cap(x, cap_value=30.0)
        return x

model = MyModel().to("mps")
```

## Notes

- First run compiles backend (takes ~10 seconds)
- Subsequent runs are instant (uses cached extension)
- Compilation output shown with `verbose=True` in `_load_backend()`
- Extension cached in `~/Library/Caches/torch_extensions/`

## Status

✅ **Working** - All tests pass on Apple Silicon with PyTorch MPS

