#!/usr/bin/env python
"""
EmberCoach ‚Äî Teaching linter for numerical precision in GPU computing

WHY THIS EXISTS:
Running 300M ops/sec for 24 hours = 25.9 trillion operations. Each float32 operation
introduces ~6e-8 relative error. Even "tiny" differences compound. This tool teaches
you where precision breaks and how to fix it.

WHAT IT TEACHES:
- FFT normalization (exactly one 1/n across rfft/irfft pairs)
- Python scalar hygiene (why float()/int() break graphs and add rounding)
- Device/dtype consistency (avoid CPU hops that add extra rounding)
- When you need extended-precision kernels
- Backend tensor operations (torch.add vs +, mx.multiply vs *)

DOCUMENTATION:
- Deep dive: docs/NUMERICAL_PRECISION_GUIDE.md
- Findings: docs/NUMERIC_STABILITY_TORCH_vs_MLX.md

Usage:
  python tools/embercoach.py <file-or-dir> [more files/dirs]

Exit code 1 if errors found (strict enforcement of precision rules).
"""

from __future__ import annotations

import argparse
import ast
import pathlib
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple


@dataclass
class Finding:
    path: pathlib.Path
    line: int
    kind: str  # 'teach' | 'warn' | 'error'
    code: str  # e.g. FFT-NORM-001
    msg: str
    why: str  # The deeper explanation


# Teaching messages with WHY context
TEACHINGS = {
    'FFT-NORM-001': {
        'title': 'FFT Normalization Contract',
        'why': """
The convolution theorem requires EXACTLY ONE 1/N factor across forward/inverse:
- PyTorch norm='forward': forward divides by N, inverse doesn't
- PyTorch norm='backward' (default): forward doesn't, inverse divides by N
- MLX default: forward doesn't, inverse divides by N

Applying 1/N twice (or zero times) gives order-one amplitude errors.
Over billions of ops, even small normalization mistakes compound.

See: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß "FFT Normalization"
""",
        'fix_torch': """
If using irfft(norm='forward'):
    k_f = torch.fft.rfft(k, n=2*L) / (2*L)  # divide spectrum once
    y = torch.fft.irfft(u_f * k_f, n=2*L, norm='forward')  # no inverse scaling

If using irfft() default:
    k_f = torch.fft.rfft(k, n=2*L)  # don't divide
    y = torch.fft.irfft(u_f * k_f, n=2*L)  # inverse applies 1/N
""",
        'fix_mlx': """
MLX default (inverse scales by 1/N):
    k_f = mx.fft.rfft(k, n=2*L)  # don't divide
    y = mx.fft.irfft(u_f * k_f, n=2*L)  # inverse applies 1/N

Never mix conventions or you get double/missing scaling!
"""
    },

    'PYTHON-SCALAR': {
        'title': 'Python Scalars Break Precision',
        'why': """
When you write `y = x * 0.5` where x is a tensor:
1. Python 0.5 is float64
2. Framework either:
   - Promotes x to float64 (expensive, wrong precision)
   - Demotes 0.5 to float32 (adds a rounding step)
   - Breaks lazy graph and computes immediately
3. Result: TWO roundings instead of ONE

Over 25 trillion ops (300M/sec √ó 24hr), these extra roundings accumulate.
Also breaks Metal buffer links in MLX (destroys gradient tracking).

See: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß "Python Scalars in Tensor Math"
""",
        'fix_torch': """
Create device-bound scalars:
    half = torch.tensor(0.5, dtype=torch.float32, device=x.device)
    y = torch.mul(x, half)  # or x * half if you must use operator

Use backend ops:
    torch.add(), torch.multiply(), torch.divide()
    (not Python +, *, /)
""",
        'fix_mlx': """
Create typed MLX scalars:
    half = mx.array(0.5, dtype=mx.float32)
    y = mx.multiply(x, half)

Use backend ops:
    mx.add(), mx.multiply(), mx.divide(), mx.power()
    (not Python +, *, /, **)
"""
    },

    'ITEM-NUMPY': {
        'title': 'Graph Breaks Destroy Precision',
        'why': """
When you call .item(), .numpy(), float(), or int() on a tensor:
1. Forces GPU‚ÜíCPU copy
2. Evaluates lazy graph immediately (loses fusion opportunities)
3. Destroys Metal buffer link (MLX can't track gradients)
4. Adds host rounding: tensor ‚Üí Python scalar ‚Üí back to tensor
5. Cannot be fused with subsequent ops

Each round-trip adds noise. Over billions of ops, this compounds to visible drift.

See: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß ".item(), .numpy(), float(), int() Conversions"
""",
        'fix_torch': """
Keep values as tensors:
    # Bad:
    if similarity.item() > threshold:

    # Good:
    threshold_t = torch.tensor(threshold, dtype=torch.float32, device=similarity.device)
    if torch.greater(similarity, threshold_t):

Store tensors in metadata, not Python scalars.
""",
        'fix_mlx': """
Keep values as MLX arrays:
    # Bad:
    if float(similarity) > threshold:

    # Good:
    threshold_a = mx.array(threshold, dtype=mx.float32)
    if mx.greater(similarity, threshold_a):

MLX is lazy ‚Äî breaking that loses the entire optimization pipeline.
"""
    },

    'NUMPY-FFT': {
        'title': 'NumPy FFT Silently Promotes to float64',
        'why': """
numpy.fft.rfft/irfft ALWAYS promotes float32 ‚Üí float64 and runs on CPU.
Then rounds back to float32 when you convert to GPU tensor.

This means:
1. Wrong precision path (float64 when you want float32)
2. CPU hop (destroys GPU pipeline)
3. Extra rounding on the way back
4. Impossible to fuse with GPU ops

NumPy is fine for final comparisons (tests), but NEVER in compute paths.

See: docs/NUMERIC_STABILITY_TORCH_vs_MLX.md ¬ß "NumPy Promotion"
""",
        'fix': """
Replace numpy.fft with backend FFT:
    # Bad:
    y_np = np.fft.rfft(x_np)

    # Good (PyTorch):
    y = torch.fft.rfft(x)

    # Good (MLX):
    y = mx.fft.rfft(x)

NumPy acceptable ONLY for final test comparisons (not compute).
"""
    },

    'DEVICE-HOP': {
        'title': 'Device Hops Add Extra Rounding',
        'why': """
When tensors live on different devices (CPU/GPU) or you call .cpu()/.to() mid-graph:
1. Framework copies data between devices
2. May change memory layout (stride/padding)
3. Each copy can introduce new rounding
4. Loses kernel fusion opportunities
5. Creates implicit synchronization points

Keep ALL tensors on ONE device through hot computation paths.

See: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß "Device Hops and Hidden Copies"
""",
        'fix': """
Create all constants on target device from the start:
    # PyTorch:
    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    x = torch.tensor(data, dtype=torch.float32, device=device)
    bias = torch.tensor(0.1, dtype=torch.float32, device=device)

    # MLX (defaults to GPU):
    x = mx.array(data, dtype=mx.float32)
    bias = mx.array(0.1, dtype=mx.float32)

Avoid .cpu()/.to() unless absolutely necessary (e.g., final logging).
"""
    },

    'EXTENDED-PRECISION': {
        'title': 'When You Need Extended Precision Kernels',
        'why': """
Standard float32 pipeline rounds at EVERY operation:
    Input ‚Üí FFT (round) ‚Üí Multiply (round) ‚Üí IFFT (round) ‚Üí Bias (round) ‚Üí Output

Four rounding points compound errors. For long-running workloads (billions of ops),
use extended precision (double-double) to round ONCE at the end:
    Input ‚Üí upcast to dd ‚Üí FFT (dd) ‚Üí Multiply (dd) ‚Üí IFFT (dd) ‚Üí round ONCE ‚Üí Output

Apple M-series GPUs don't have native float64, but we can emulate ~32 decimal digits
using two float32 values (hi, lo) with error-free transforms.

See: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß "The Solution: Extended Precision"
""",
        'when': """
Consider extended precision when:
- FFT-based convolutions (frequency-domain multiply most sensitive)
- Depthwise convolutions (small kernels, many iterations)
- Long reductions/accumulations
- Running >10^9 operations on same state
- Need bit-reproducibility across runs

Available via HyperProfile flags:
    ep_freqmul=true    # dd complex multiply (cheap, high leverage)
    ep_depthwise=true  # dd depthwise conv (cheap)
    ep_fft=true        # dd butterfly (heavier, for extreme stability)
""",
        'kernels': """
Precision kernels available in:
    experimental/metal_bitexact/ComplexMul.metal  (strict FP32, will extend to dd)
    experimental/metal_bitexact/Depthwise3.metal  (deterministic 3-tap)

To use:
    1. Set HyperProfile flag (e.g., ep_freqmul=true in profiles/torch_like.json)
    2. Extended-precision kernels replace standard ops automatically
    3. No code changes needed beyond profile selection
"""
    }
}


class Coach(ast.NodeVisitor):
    def __init__(self, src: str, path: pathlib.Path) -> None:
        self.src = src
        self.path = path
        self.alias_torch: Optional[str] = None
        self.alias_mx: Optional[str] = None
        self.alias_np: Optional[str] = None
        self.findings: List[Finding] = []
        self.stack: List[ast.AST] = []

        # Track usage patterns to give deeper advice
        self.has_fft_rfft = False
        self.has_fft_irfft = False
        self.has_complex_multiply = False
        self.python_scalar_count = 0

    def _add(self, node: ast.AST, kind: str, code: str, msg: str, why: str = '') -> None:
        self.findings.append(Finding(self.path, getattr(node, 'lineno', 1), kind, code, msg, why))

    def _is_name(self, node: ast.AST, name: str) -> bool:
        return isinstance(node, ast.Name) and node.id == name

    def _matches_fft(self, node: ast.Call, which: str) -> bool:
        f = node.func
        if isinstance(f, ast.Attribute):
            if (isinstance(f.value, ast.Attribute)
                    and isinstance(f.value.value, ast.Name)
                    and f.attr == which
                    and ((self.alias_torch and f.value.value.id == self.alias_torch and f.value.attr == 'fft')
                         or (self.alias_mx and f.value.value.id == self.alias_mx and f.value.attr == 'fft'))):
                return True
        return False

    def _get_kw(self, node: ast.Call, name: str) -> Optional[ast.AST]:
        for kw in node.keywords:
            if kw.arg == name:
                return kw.value
        return None

    def visit_Import(self, node: ast.Import) -> None:
        for alias in node.names:
            if alias.name == 'torch':
                self.alias_torch = alias.asname or 'torch'
            elif alias.name in ('mlx.core', 'mlx'):
                self.alias_mx = alias.asname or ('mx' if alias.name == 'mlx.core' else 'mlx')
            elif alias.name == 'numpy':
                self.alias_np = alias.asname or 'np'
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
        mod = node.module or ''
        if mod.startswith('torch'):
            self.alias_torch = 'torch'
        elif mod.startswith('mlx'):
            self.alias_mx = 'mx'
        elif mod == 'numpy':
            self.alias_np = 'np'
        self.generic_visit(node)

    def visit(self, node: ast.AST):
        self.stack.append(node)
        super().visit(node)
        self.stack.pop()

    def _inside_indexing(self) -> bool:
        return any(isinstance(n, ast.Subscript) for n in self.stack)

    def _backend_hint(self) -> str:
        if self.alias_mx and not self.alias_torch:
            return TEACHINGS['PYTHON-SCALAR']['fix_mlx']
        if self.alias_torch and not self.alias_mx:
            return TEACHINGS['PYTHON-SCALAR']['fix_torch']
        return "Use backend tensor scalars (torch.tensor or mx.array) and backend math ops."

    def visit_Call(self, node: ast.Call) -> None:
        # Track FFT usage for final summary
        if self._matches_fft(node, 'rfft'):
            self.has_fft_rfft = True
            lib = 'torch' if self.alias_torch else ('mlx' if self.alias_mx else 'lib')
            fix = TEACHINGS['FFT-NORM-001']['fix_torch'] if self.alias_torch else TEACHINGS['FFT-NORM-001']['fix_mlx']
            self._add(
                node, 'teach', 'FFT-NORM-001',
                f"üìö {lib}.fft.rfft detected ‚Äî normalization check required",
                TEACHINGS['FFT-NORM-001']['why'] + '\n' + fix
            )

        if self._matches_fft(node, 'irfft'):
            self.has_fft_irfft = True
            norm = self._get_kw(node, 'norm')
            norm_val = None
            if isinstance(norm, ast.Constant) and isinstance(norm.value, str):
                norm_val = norm.value

            msg = "üìö irfft normalization: "
            if norm_val == 'forward':
                msg += "norm='forward' means inverse does NOT scale. Ensure rfft path divides spectrum by n."
            elif norm_val in ('backward', None):
                msg += "default/backward means inverse scales by 1/n. Do NOT pre-divide spectrum."
            else:
                msg += "verify normalization contract (exactly one 1/n across pair)."

            self._add(node, 'teach', 'FFT-NORM-002', msg, TEACHINGS['FFT-NORM-001']['why'])

        # NumPy FFT
        if isinstance(node.func, ast.Attribute) and isinstance(node.func.value, ast.Name):
            base = node.func.value.id
            attr = node.func.attr
            if self.alias_np and base == self.alias_np and attr in ('fft', 'rfft', 'irfft', 'fftn', 'irfftn'):
                self._add(
                    node, 'error', 'NUMPY-FFT-001',
                    "‚ùå numpy.fft promotes float32‚Üífloat64 and runs on CPU",
                    TEACHINGS['NUMPY-FFT']['why'] + '\n' + TEACHINGS['NUMPY-FFT']['fix']
                )

            # Python numerics passed to backend ops
            if base in ((self.alias_torch or ''), (self.alias_mx or '')) and attr in (
                    'add', 'subtract', 'multiply', 'divide', 'power', 'pow', 'matmul', 'einsum'
            ):
                for arg in node.args:
                    if isinstance(arg, ast.Constant) and isinstance(arg.value,
                                                                    (int, float)) and not self._inside_indexing():
                        self.python_scalar_count += 1
                        self._add(
                            node, 'error', 'PYTHON-SCALAR-001',
                            f"‚ùå Python numeric literal passed to {base}.{attr}",
                            TEACHINGS['PYTHON-SCALAR']['why'] + '\n' + self._backend_hint()
                        )

        # .item() / .numpy() / float()/int()
        if isinstance(node.func, ast.Attribute):
            if node.func.attr in ('item', 'numpy'):
                self._add(
                    node, 'error', 'GRAPH-BREAK-001',
                    f"‚ùå .{node.func.attr}() breaks lazy graph, forces GPU‚ÜíCPU copy",
                    TEACHINGS['ITEM-NUMPY']['why'] + '\n' +
                    (TEACHINGS['ITEM-NUMPY']['fix_torch'] if self.alias_torch else TEACHINGS['ITEM-NUMPY']['fix_mlx'])
                )
            elif node.func.attr in ('cpu', 'to'):
                self._add(
                    node, 'warn', 'DEVICE-HOP-001',
                    f"‚ö†Ô∏è  Device hop (.{node.func.attr}) adds extra rounding",
                    TEACHINGS['DEVICE-HOP']['why'] + '\n' + TEACHINGS['DEVICE-HOP']['fix']
                )

        # float()/int() casts
        if isinstance(node.func, ast.Name) and node.func.id in ('float', 'int'):
            self._add(
                node, 'error', 'CAST-001',
                f"‚ùå Python {node.func.id}() cast breaks graph and adds host rounding",
                TEACHINGS['ITEM-NUMPY']['why'] + '\n' +
                (TEACHINGS['ITEM-NUMPY']['fix_torch'] if self.alias_torch else TEACHINGS['ITEM-NUMPY']['fix_mlx'])
            )

        self.generic_visit(node)

    def visit_BinOp(self, node: ast.BinOp) -> None:
        lit = (isinstance(node.left, ast.Constant) and isinstance(node.left.value, (int, float))) or \
              (isinstance(node.right, ast.Constant) and isinstance(node.right.value, (int, float)))
        if lit and not self._inside_indexing():
            self.python_scalar_count += 1
            self._add(
                node, 'error', 'PYTHON-SCALAR-002',
                "‚ùå Python numeric in tensor expression (use backend ops)",
                TEACHINGS['PYTHON-SCALAR']['why'] + '\n' + self._backend_hint()
            )
        self.generic_visit(node)

    def _report_assign_const(self, value: ast.AST, node: ast.AST) -> None:
        if isinstance(value, ast.Constant) and isinstance(value.value, (int, float)):
            hint = self._backend_hint()
            if self.alias_mx and isinstance(value.value, float):
                hint = "Use mx.array(0.1, dtype=mx.float32) instead of bare float."
            self._add(
                node, 'error', 'ASSIGN-SCALAR-001',
                "‚ùå Bare Python scalar in assignment",
                TEACHINGS['PYTHON-SCALAR']['why'] + '\n' + hint
            )

    def visit_Assign(self, node: ast.Assign) -> None:
        self._report_assign_const(node.value, node)
        self.generic_visit(node)

    def visit_AnnAssign(self, node: ast.AnnAssign) -> None:
        if node.value is not None:
            self._report_assign_const(node.value, node)
        self.generic_visit(node)


def scan_path(p: pathlib.Path) -> Tuple[List[Finding], Dict[str, int]]:
    """Returns (findings, usage_stats)"""
    findings: List[Finding] = []
    files: List[pathlib.Path] = []
    stats = {'files': 0, 'has_fft': 0, 'python_scalars': 0}

    if p.is_dir():
        files = [x for x in p.rglob('*.py')]
    elif p.suffix == '.py':
        files = [p]

    for f in files:
        try:
            src = f.read_text(encoding='utf-8')
        except Exception:
            continue
        try:
            tree = ast.parse(src)
        except SyntaxError:
            continue

        stats['files'] += 1
        c = Coach(src, f)
        c.visit(tree)
        findings.extend(c.findings)

        if c.has_fft_rfft or c.has_fft_irfft:
            stats['has_fft'] += 1
        stats['python_scalars'] += c.python_scalar_count

    return findings, stats


def print_summary(stats: Dict[str, int], findings: List[Finding]):
    """Print educational summary based on what was found"""
    print("\n" + "=" * 80)
    print("EMBERCOACH SUMMARY")
    print("=" * 80)
    print(f"Scanned {stats['files']} files")
    print(f"Found {len(findings)} teaching moments")

    errors = sum(1 for f in findings if f.kind == 'error')
    warns = sum(1 for f in findings if f.kind == 'warn')
    teaches = sum(1 for f in findings if f.kind == 'teach')

    print(f"  ‚ùå {errors} errors (strict precision violations)")
    print(f"  ‚ö†Ô∏è  {warns} warnings (drift risks)")
    print(f"  üìö {teaches} teaching tips (good practices)")

    if stats['has_fft'] > 0:
        print(f"\nüî¨ FFT Usage Detected in {stats['has_fft']} files")
        print("   Consider extended-precision kernels for long-running workloads:")
        print("   " + TEACHINGS['EXTENDED-PRECISION']['when'])

    if stats['python_scalars'] > 0:
        print(f"\n‚ö° {stats['python_scalars']} Python scalar usage patterns")
        print("   At 300M ops/sec √ó 24hr, these extra roundings compound significantly.")
        print("   Review: docs/NUMERICAL_PRECISION_GUIDE.md ¬ß 'Python Scalars'")

    print("\nüìñ For deep explanations:")
    print("   docs/NUMERICAL_PRECISION_GUIDE.md  (full tutorial)")
    print("   docs/NUMERIC_STABILITY_TORCH_vs_MLX.md  (findings summary)")
    print("=" * 80 + "\n")


def main():
    ap = argparse.ArgumentParser(
        description='EmberCoach: Teaching linter for GPU numerical precision',
        epilog='Teaches WHY precision matters and HOW to fix issues. See docs/ for details.'
    )
    ap.add_argument('paths', nargs='+', help='Files or directories to scan')
    ap.add_argument('--verbose', '-v', action='store_true', help='Show full WHY context for each finding')
    args = ap.parse_args()

    all_findings = []
    combined_stats = {'files': 0, 'has_fft': 0, 'python_scalars': 0}

    for p in args.paths:
        findings, stats = scan_path(pathlib.Path(p))
        all_findings.extend(findings)
        for k in combined_stats:
            combined_stats[k] += stats[k]

    # Print findings
    for f in all_findings:
        icon = {'error': '‚ùå', 'warn': '‚ö†Ô∏è ', 'teach': 'üìö'}.get(f.kind, '  ')
        print(f"{f.path}:{f.line}: [{icon} {f.code}] {f.msg}")
        if args.verbose and f.why:
            print(f"   WHY: {f.why}")
            print()

    # Summary
    if all_findings:
        print_summary(combined_stats, all_findings)
    else:
        print("‚úÖ EmberCoach: No precision issues found ‚Äî excellent!")

    # Exit code
    errors = sum(1 for f in all_findings if f.kind == 'error')
    if errors:
        raise SystemExit(1)


if __name__ == '__main__':
    main()
