We present a practical, MPS‑native path to long‑context xLSTM inference on Apple Silicon by replacing monolithic GPU kernels with many small, compiled PyTorch kernels and coordinating them via a multi‑kernel scheduler. Our approach preserves strict recurrence within each head‑band while exploiting cross‑band and hierarchical concurrency with a thread‑queued driver and Ray actors in local mode. On an M3 Ultra, we achieve stable, GPU‑only execution with competitive decode throughput (~25 tok/s) and strong prefill rates across 2K–32K contexts, without relying on Triton or custom Metal shaders. We introduce an optimizer with genetic/random/bandit exploration and full observability (JSONL/CSV logs, best‑of‑breed capture), and outline an extended‑context roadmap based on hierarchical prefill (chunk→medium→long summaries), working‑memory pools (recent/relevant/compressed/persistent), and dynamic context allocation. This design avoids Metal argument‑limit codegen failures, maintains exact numerics for recurrence, and provides a viable foundation for 20K–100K+ context scaling on consumer Apple GPUs.
