# Metal Kernel Guide for xLSTM

This comprehensive guide covers Metal kernel implementation for xLSTM operations across MLX and PyTorch frameworks.

## Table of Contents
1. [MLX Metal Kernels](#mlx-metal-kernels)
2. [PyTorch MPS Backend](#pytorch-mps-backend)
3. [PyTorch Custom Metal Extensions](#pytorch-custom-metal-extensions)
4. [PyTorch JIT and Metal](#pytorch-jit-and-metal)
5. [HPC Limb Technique](#hpc-limb-technique)
6. [MPS Tensor Buffer Access Problem](#mps-tensor-buffer-access-problem)
7. [Common Patterns](#common-patterns)

---

## MLX Metal Kernels

### Key Concepts:
1. **Use @mx.custom_function decorator** for clean interfaces
2. **Metal kernels provide function body only** - MLX generates signatures
3. **Input naming**: `inp0`, `inp1`, etc. with `inp0_shape[0]` for dimensions
4. **Output naming**: `out0`, `out1`, etc.
5. **Thread positioning**: `thread_position_in_grid.x` for global position
6. **Synchronization**: `threadgroup_barrier(mem_flags::mem_device)` for dependencies
7. **Numerical stability**: Guard against division by zero, NaN propagation

### MLX Kernel Example:
```python
import mlx.core as mx

kernel_source = """
    uint tid = thread_position_in_grid.x;
    if (tid < inp0_shape[0]) {
        float val = inp0[tid];
        float cap = inp1[0];
        out0[tid] = cap * tanh(val / cap);  // Soft capping
    }
"""

soft_cap_kernel = mx.fast.metal_kernel(
    name="soft_cap",
    source=kernel_source,
    input_names=["input", "cap_value"],
    output_names=["output"],
)
```

---

## PyTorch MPS Backend

### Architecture:
- PyTorch uses **Metal Performance Shaders (MPS)** for GPU acceleration on Apple Silicon
- Located in `aten/src/ATen/native/mps/` in PyTorch repo
- Uses `.mm` (Objective-C++) files, NOT `.metal` files
- Metal shader code is embedded as string literals

### Key Differences from Swift/iOS:
| Aspect | Swift/iOS Projects | PyTorch MPS |
|--------|-------------------|-------------|
| File Extension | `.metal` | `.mm` (Objective-C++) |
| Shader Code | Separate files | String literals in C++ |
| Compilation | `xcrun metal` | Runtime via `newLibraryWithSource` |
| Loading | Bundle resources | Dynamic compilation |

### PyTorch MPS Pattern:
```objective-c
// In a .mm file
const char* metal_kernel_source = R"METAL(
    #include <metal_stdlib>
    using namespace metal;
    
    kernel void soft_cap_kernel(
        device float* input [[buffer(0)]],
        device float* output [[buffer(1)]],
        constant float& cap_value [[buffer(2)]],
        uint tid [[thread_position_in_grid]]
    ) {
        output[tid] = cap_value * tanh(input[tid] / cap_value);
    }
)METAL";

// Compile at runtime
NSString* code = [NSString stringWithUTF8String:metal_kernel_source];
id<MTLLibrary> library = [device newLibraryWithSource:code 
                                              options:nil 
                                                error:&error];
```

---

## PyTorch Custom Metal Extensions

### Build System Requirements:

1. **File Structure**:
```
pytorch_metal_extension/
├── setup.py
├── src/
│   └── my_kernel.mm       # Objective-C++ with embedded Metal
└── python/
    └── wrapper.py          # Python bindings
```

2. **setup.py Configuration**:
```python
from setuptools import setup, Extension
from torch.utils.cpp_extension import BuildExtension

ext_modules = [
    Extension(
        'my_metal_ops',
        ['src/my_kernel.mm'],
        extra_compile_args=['-x', 'objective-c++', '-std=c++17'],
        extra_link_args=['-framework', 'Metal', '-framework', 'Foundation'],
    )
]

setup(
    ext_modules=ext_modules,
    cmdclass={'build_ext': BuildExtension}
)
```

3. **Compilation Commands**:
```bash
# For standalone .metal files (if needed)
xcrun metal -c kernel.metal -o kernel.air
xcrun metallib kernel.air -o kernel.metallib

# For .mm files with PyTorch
python setup.py install
```

### Common Build Issues:

| Issue | Cause | Solution |
|-------|-------|----------|
| `.mm` file not recognized | setuptools limitation | Use `-x objective-c++` flag |
| `torch/extension.h` not found | Missing include paths | Add `torch.utils.cpp_extension.include_paths()` |
| Metal frameworks not linked | Missing link flags | Add `-framework Metal` etc. |
| torch not available in setup | PEP 517 isolation | Use runtime compilation approach |

---

## PyTorch JIT and Metal

### Current State (2024):
- **TorchScript**: Incompatible with dynamic RNN shapes
- **torch.compile**: Early prototype for Metal, limited to <64 buffers
- **Eager mode**: Most stable for Metal/MPS operations

### torch.compile Metal Limitations:

```python
# This will fail with >64 buffer error
@torch.compile(backend="inductor")
def complex_lstm(x, hidden_states):
    # Complex operations generate too many buffers
    pass

# Workaround: Use HPC limb technique to pack data
```

### JIT Compilation Issues:
```python
# TorchScript fails on dynamic shapes
traced = torch.jit.trace(model, example_input)  # Shape mismatch errors

# Better approach: torch.compile with dynamic=True (when Metal support improves)
compiled = torch.compile(model, dynamic=True, backend="inductor")
```

### Metal Buffer Exhaustion:
- Metal has a hard limit of 64 buffers per kernel
- torch.compile can generate kernels exceeding this limit
- Solution: HPC limb technique (see below)

---

## HPC Limb Technique

### Purpose:
1. **Work around Metal's 64-buffer limit**
2. **Emulate float64 precision** (Metal only supports float32/float16)
3. **Reduce memory usage** with packed representations

### Implementation:
```metal
#define NUM_LIMBS 8u          // 8 × 16-bit = 128-bit precision
#define LIMB_RADIX 65536.0f   // 2^16

// Pack multiple values into limbs
uint limbs[NUM_LIMBS] = {0u};
for (uint i = 0; i < data_size; i++) {
    uint bits = as_type<uint>(data[i]);
    ushort lo = bits & 0xFFFFu;
    ushort hi = (bits >> 16) & 0xFFFFu;
    
    limbs[0] += uint(lo * lo) & 0xFFFFu;
    limbs[1] += (uint(lo * lo) >> 16) + uint(lo * hi);
    // ... carry propagation
}

// Convert back to float
float result = 0.0f;
float radix = 1.0f;
for (uint l = 0; l < NUM_LIMBS; l++) {
    result += float(limbs[l]) * radix;
    radix *= LIMB_RADIX;
}
```

### Benefits:
- Reduces buffer count from 64+ to ~12
- Provides 128-bit precision on hardware with only float32
- Enables complex computations within Metal constraints

---

## Common Patterns

### xLSTM-Specific Optimizations:

| Operation | Metal Pattern | Notes |
|-----------|--------------|-------|
| **Soft capping** | `cap * tanh(x / cap)` | Prevents gradient explosion |
| **Matrix memory** | Outer products for mLSTM | Use einsum-like patterns |
| **Sequential deps** | Single-threaded sections | Can't parallelize time steps |
| **Causal conv** | Masked operations | Only past/current visible |
| **RMSNorm** | Parallel reduction | Use threadgroup shared memory |

### Performance Tips:

1. **Memory Coalescing**: Access contiguous memory patterns
2. **Warp Efficiency**: Use multiples of 32 threads
3. **Register Pressure**: Minimize local variables
4. **Shared Memory**: Use threadgroup memory for reductions
5. **Kernel Fusion**: Combine operations to reduce launches

### Debugging Metal Kernels:

```python
# MLX debugging
mx.set_default_device(mx.gpu)
output = kernel(inputs, grid=(1024,1,1), threadgroup=(32,1,1))
mx.eval(output)  # Force synchronous execution

# PyTorch MPS debugging
torch.mps.synchronize()  # Ensure kernel completes
torch.mps.profiler.start()  # Profile performance
torch.mps.profiler.stop()
```

---

## MPS Tensor Buffer Access: The Complete Truth

### ❌ The Wrong Way (Kludgy CPU Workaround)

**Problem**: Many examples show CPU copying as the "only way" to access MPS tensor data:

```cpp
// This is SLOW and UNNECESSARY
auto cpu_input = input.cpu();  // GPU -> CPU copy
id<MTLBuffer> buffer = [device newBufferWithBytes:cpu_input.data_ptr()...];  // CPU -> GPU copy
// Run Metal kernel
auto result_cpu = torch::empty_like(cpu_input);
memcpy(result_cpu.data_ptr(), [output_buffer contents]...);  // GPU -> CPU copy  
return result_cpu.to(input.device());  // CPU -> GPU copy
```

**Why this seemed necessary**: 
- Using wrong storage API (`tensor.data_ptr()` vs `tensor.storage().data()`)
- Incorrect casting (`__bridge` vs `__builtin_bit_cast`)
- Missing knowledge of PyTorch's internal MPS patterns

### ✅ The RIGHT Way (Direct Buffer Access)

**Solution**: Use PyTorch's internal `__builtin_bit_cast` pattern for **direct GPU buffer access**:

```cpp
// PyTorch's ACTUAL internal method for MPS operations
static inline id<MTLBuffer> getMTLBufferStorage(const at::Tensor& tensor) {
    TORCH_CHECK(tensor.is_mps(), "Tensor must be on MPS device");
    TORCH_CHECK(tensor.is_contiguous(), "Tensor must be contiguous");
    
    // Direct bit cast - NO CPU COPYING!
    return __builtin_bit_cast(id<MTLBuffer>, tensor.storage().data());
}

torch::Tensor softcap_direct(torch::Tensor input, float cap_value) {
    @autoreleasepool {
        // Ensure proper format
        if (!input.is_contiguous()) input = input.contiguous();
        if (input.dtype() != torch::kFloat32) input = input.to(torch::kFloat32);
        
        auto output = torch::empty_like(input);
        
        // DIRECT MTLBuffer access - like MLX!
        id<MTLBuffer> input_buffer = getMTLBufferStorage(input);
        id<MTLBuffer> output_buffer = getMTLBufferStorage(output);
        
        // Run Metal kernel directly on GPU buffers
        // ... Metal kernel execution ...
        
        return output;  // Already on correct device
    }
}
```

### Performance Comparison

**Benchmark Results** (10,000 element tensor, 100 iterations):
- **Direct access**: 0.019s
- **CPU copying**: 0.081s  
- **Speedup**: **4.2x faster**

### Why Both MLX and PyTorch Support Direct Access

**MLX Pattern**:
```python
# MLX: Direct array passing to Metal kernel
kernel(inputs=[x, cap_array], ...)  # No copying!
```

**PyTorch Pattern**:
```cpp
// PyTorch: Direct MTLBuffer access (internal method)
id<MTLBuffer> buf = __builtin_bit_cast(id<MTLBuffer>, tensor.storage().data());
```

**Both frameworks avoid CPU copying** - they access GPU buffers directly.

### Common Misconceptions Debunked

❌ **"PyTorch MPS requires CPU copying"** → **FALSE**  
✅ PyTorch's internal MPS operations use direct buffer access

❌ **"MLX is faster because it avoids CPU copying"** → **MISLEADING**  
✅ Both MLX and PyTorch support direct GPU buffer access

❌ **"tensor.data_ptr() gives you the MTLBuffer"** → **FALSE**  
✅ Use `tensor.storage().data()` with proper bit casting

❌ **"Direct buffer access is unsafe/unreliable"** → **FALSE**  
✅ It's how PyTorch's own MPS operations work internally

### The Real API Difference

| Framework | Buffer Access Method |
|-----------|---------------------|
| **MLX** | `array.data<T>()` returns Metal buffer directly |
| **PyTorch** | `__builtin_bit_cast(id<MTLBuffer>, tensor.storage().data())` |

**Key Insight**: MLX appears simpler because it's designed from the ground up for Metal. PyTorch requires bit casting through its storage abstraction layer, but **both achieve the same result** - direct GPU buffer access.

### When CPU Copying Actually Makes Sense

Use CPU copying only when:
1. **Debugging** - Explicit data flow is easier to debug
2. **Complex tensor layouts** - Non-contiguous tensors with complex strides
3. **API compatibility** - Working with legacy code that expects CPU data
4. **Safety first** - When prototyping before optimizing

**For production PyTorch Metal extensions, use direct buffer access.**

### Bottom Line: The Kludgy Workaround is Unnecessary

**The truth**: Both MLX and PyTorch support direct GPU buffer access without CPU copying. The key is using the correct storage API and bit casting pattern that PyTorch's internal MPS operations use.

**Performance impact**: Direct access is 4.2x faster than CPU copying.

**Takeaway**: If you can pass tensors back and forth in MLX without CPU copying, you can do the same in PyTorch - you just need to use the right internal pattern.

### Relationship to HPC Limb Technique

⚠️ **Important**: The HPC limb technique solves a **completely different problem** from buffer access:

| Problem | Solution | Purpose |
|---------|----------|---------|
| **Buffer Access** | Direct bit casting with `__builtin_bit_cast` | Avoid CPU copying for performance |
| **HPC Limbs** | 16-bit limb arithmetic for precision | Emulate float64 precision in Metal |

**HPC Limb Technique Overview**:
```cpp
// From ember-ml: 16-bit limb arithmetic for high precision
#define NUM_LIMBS   8u          // 128-bit accumulator (8 × 16-bit)
#define LIMB_RADIX  65536.0f    // 2^16

// Use limbs when Metal's float32 precision isn't enough
float limb_accumulator[NUM_LIMBS];  // Higher precision than float64
```

**When to use each**:
- **Direct buffer access**: Always (for performance)  
- **HPC limbs**: Only when you need >float32 precision in Metal kernels

**They work together**:
```cpp
torch::Tensor high_precision_operation(torch::Tensor input) {
    // 1. Direct buffer access (no CPU copying)
    id<MTLBuffer> input_buffer = getMTLBufferStorage(input);
    id<MTLBuffer> output_buffer = getMTLBufferStorage(output);
    
    // 2. Use HPC limb kernel for high precision computation
    // Metal kernel uses 16-bit limb arithmetic internally
    // but still accesses buffers directly from MPS tensors
}
```

**Both techniques are optimizations** that can be combined for maximum performance and precision.

### Third Orthogonal Optimization: Non-Square Matrix Support

⚡ **Advanced**: ember-ml goes beyond what Metal normally supports with **orthogonal non-square matrices**:

| Metal Limitation | ember-ml Solution | Benefit |
|------------------|-------------------|---------|
| **Square matrix bias** | Orthogonal non-square algorithm | Works with any matrix shape |
| **Poor conditioning** | HPC limb arithmetic + orthogonalization | Numerical stability for ill-conditioned matrices |
| **Memory constraints** | Block-based tiled computation | Handles very large matrices |

**The Challenge**: Standard Metal operations struggle with:
- **Non-square matrices** (e.g., 1000×50, 10×500)
- **Ill-conditioned matrices** (exponentially decreasing singular values)
- **Very rectangular matrices** (extreme aspect ratios)

**ember-ml's Solution**: 
```cpp
// Metal kernel for orthogonal non-square matrices
kernel void orthogonal_nonsquare_kernel(
    device float* A [[buffer(0)]],
    device float* out [[buffer(1)]],
    constant uint* shape [[buffer(2)]],
    constant uint* block_param [[buffer(3)]],
    uint3 thread_position_in_grid [[thread_position_in_grid]]
) {
    uint m = shape[0];  // Rows (can be >> cols)
    uint n = shape[1];  // Cols (can be << rows)
    
    // Block-based orthogonalization for any matrix shape
    uint block_size = block_param[0];
    uint thread_id = thread_position_in_grid.x;
    uint block_id = thread_position_in_grid.y;
    
    // Extended precision accumulation for numerical stability
    float norm_sq_high = 0.0f;
    float norm_sq_low = 0.0f;
    
    for (uint i = 0; i < m; i++) {
        float val = curr_col[i];
        float val_sq = val * val;
        
        // Kahan summation for precision
        float t = norm_sq_high + val_sq;
        float e = (norm_sq_high - t) + val_sq;
        norm_sq_high = t;
        norm_sq_low += e;
    }
    // ... Gram-Schmidt with extended precision
}
```

**Test Results**: 
- **Handles extreme shapes**: 1000×10, 10×1000, even 3×100
- **Better numerical stability** than standard QR for ill-conditioned matrices
- **Performance**: Faster than standard implementations for large rectangular matrices

**All Three Techniques Combined**:
```cpp
torch::Tensor advanced_operation(torch::Tensor input) {
    // 1. Direct buffer access (4.2x performance boost)
    id<MTLBuffer> input_buffer = getMTLBufferStorage(input);
    
    // 2. HPC limb arithmetic (>float64 precision)
    // 3. Non-square orthogonal algorithm (any matrix shape)
    
    // Metal kernel uses all three optimizations:
    // - Direct GPU buffer access
    // - Extended precision arithmetic
    // - Support for non-square matrices
}
```

This makes ember-ml capable of operations that go **beyond Metal's normal limitations**.

---

## Best Practices Summary

### DO:
- ✅ Embed Metal code as strings in .mm files for PyTorch
- ✅ Use runtime compilation with `newLibraryWithSource`
- ✅ Pack data with HPC limbs to avoid buffer limits
- ✅ Test with small inputs first, scale up gradually
- ✅ Use `mx.eval()` or `torch.mps.synchronize()` for debugging

### DON'T:
- ❌ Create separate .metal files for PyTorch extensions
- ❌ Exceed 64 Metal buffers per kernel
- ❌ Assume float64 support (use HPC limbs instead)
- ❌ Use TorchScript for dynamic RNN shapes
- ❌ Rely on torch.compile Metal backend for production (2024)

---

## References

- [PyTorch MPS Backend](https://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/mps)
- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [Metal Shading Language Specification](https://developer.apple.com/metal/)
- [PyTorch Custom Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- [HPC Limb Implementation](ember-ml/mlxtests/hpc_16x8_method/)

---

*Last Updated: 2024*